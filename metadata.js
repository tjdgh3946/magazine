const imageData = [
  {
    date: "2025-07-11",
    filename: "slides/ChangeofVariable.png",
    imageCount: 1, 
    caption: "Change-of-variable rules distinguish between density transformation and measure adjustment. The entropy example illustrates how linear mappings like y = Ax affect both terms via Jacobian and determinant scaling.hange-of-variable rules distinguish between density transformation and measure adjustment. The entropy example illustrates how linear mappings like y = Ax affect both terms via Jacobian and determinant scaling.hange-of-variable rules distinguish between density transformation and measure adjustment. The entropy example illustrates how linear mappings like y = Ax affect both terms via Jacobian and determinant scaling.",
    keywords: ["change of variable", "jacobian", "density transformation", "entropy"], 
    message: "CoV 헷갈리기 nono"
  },
  {
    date: "2025-07-10",
    filename: "slides/conditionalExpectation.png",
    imageCount: 1, 
    caption: "Conditional expectation isn’t just a fancy average — it’s the L₂-minimizing oracle that projects truth onto the realm of the known.",
    keywords: ["conditional expectation", "projection theorem", "best predictor"], 
    message: "Conditional Expectation는 생각보다 어렵습니다"
  },
  {
    date: "2025-07-08",
    filename: "slides/FisherInformation.png",
    imageCount: 1, 
    caption: "Fisher information reflects the curvature of the log-likelihood, dictating the estimator’s variance via the Cramér–Rao bound. Sharper peaks imply more certainty, while flatter likelihoods signal low information and high uncertainty.",
    keywords: ["fisher information", "log-likelihood", "cramer-rao bound", "estimator variance"], 
    message: "Fisher Information 알고 있으면 좋지"
  },
  {
    date: "2025-07-12",
    filename: "slides/compressARC.png",
    imageCount: 2, 
    caption: "CompressARC performs progressive abstraction learning, evolving from copying surface-level patterns to extracting symbolic color-direction mappings—all within test-time training.",
    keywords: ["ARC", "test-time optimization", "pattern discovery"],
    message: "대충 읽었지만 정리함"
  }, 
  {
    date: "2025-07-17",
    filename: "slides/mgf.png",
    imageCount: 1, 
    caption: "랜덤변수 moment genearting function 왜 씀? 1. mean,variance 계산하기 더 쉬움. 2. 랜덤 변수 2개 더해서 나온 랜덤 변수 distribution 직번 구하지 않고도 여러가지 결과 만들 수 있음",
    keywords: ["probability", "mgf"],
    message: "랜덤변수 moment genearting function 왜 씀?"
  },
  {
    date: "2025-07-25",
    filename: "slides/greedyKCenter.png",
    imageCount: 1, 
    caption: "슬곧내 (슬라이드 내용이 곧 내용)",
    keywords: ["approximate", "greedy", "coreset"],
    message: "Greedy K-center- k개의 coreset 찾는 단순한 알고리즘"
  }  
];