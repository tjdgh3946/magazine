const imageData = [
  {
    date: "2025-07-11",
    filename: "slides/ChangeofVariable.png",
    imageCount: 1, 
    caption: "Change-of-variable rules distinguish between density transformation and measure adjustment. The entropy example illustrates how linear mappings like y = Ax affect both terms via Jacobian and determinant scaling.hange-of-variable rules distinguish between density transformation and measure adjustment. The entropy example illustrates how linear mappings like y = Ax affect both terms via Jacobian and determinant scaling.hange-of-variable rules distinguish between density transformation and measure adjustment. The entropy example illustrates how linear mappings like y = Ax affect both terms via Jacobian and determinant scaling.",
  keywords: ["change of variable", "jacobian", "density transformation", "entropy"]
  },
  {
    date: "2025-07-10",
    filename: "slides/conditionalExpectation.png",
    imageCount: 1, 
    caption: "Conditional expectation isn’t just a fancy average — it’s the L₂-minimizing oracle that projects truth onto the realm of the known.",
    keywords: ["conditional expectation", "projection theorem", "best predictor"]
  },
  {
    date: "2025-07-08",
    filename: "slides/FisherInformation.png",
    imageCount: 1, 
    caption: "Fisher information reflects the curvature of the log-likelihood, dictating the estimator’s variance via the Cramér–Rao bound. Sharper peaks imply more certainty, while flatter likelihoods signal low information and high uncertainty.",
    keywords: ["fisher information", "log-likelihood", "cramer-rao bound", "estimator variance"]
  },
  {
  date: "2025-07-12",
  filename: "slides/compressARC.png",
  imageCount: 2, 
  caption: "CompressARC performs progressive abstraction learning, evolving from copying surface-level patterns to extracting symbolic color-direction mappings—all within test-time training.",
  keywords: ["ARC", "test-time optimization", "pattern discovery"]
  }

  // ... more slides
];